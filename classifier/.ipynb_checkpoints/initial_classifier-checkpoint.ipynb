{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:33:42.877522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import statements:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Hugging face import:\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving zero-shot classification: \n",
    "theme_pipeline = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Saving sentiment pipeline\n",
    "# Ethan: specifying a model to ensure pipeline stability as per Huggingface recommendation\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data:\n",
    "df = pd.read_csv(\"/Users/amaribauer/Desktop/A_ML/FinalProject/okcupid_profiles.csv\")\n",
    "# Ethan: using a relative path for reproducability. To reproduce, add a data file in your home directory and put the profile document there.\n",
    "#df = pd.read_csv(\"../data/okcupid_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethan: sampling here rather than after data cleaning\n",
    "df_sample = df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essay dataframe:\n",
    "essays_df = df_sample.loc[:, [\"essay0\", \"essay1\", \"essay2\", \"essay3\", \"essay4\", \n",
    "                   \"essay5\", \"essay6\", \"essay7\", \"essay8\", \"essay9\"]]\n",
    "essays_df = essays_df.astype(str)\n",
    "\n",
    "# Essay0 dataframe only \"about me\":\n",
    "essay0_df = df_sample.loc[:, [\"essay0\"]]\n",
    "essay0_df = essay0_df.dropna(subset=['essay0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed Additions to labels: travel, drinking, drugs, kids\n",
    "Proposed Removals: teen, enthusiastic, time periods, avid, miscellaneous, rock, sci-fi, favorite, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting labels. Think of thse as our classes:\n",
    "# Ethan: Implemented Amari's suggestion for label names. We can tweak this more going forward.\n",
    "candidate_labels = ['TV', 'movies', 'music',\n",
    "          'comedies', 'food', 'drama',\n",
    "          'music', 'books', 'travel', 'drinking', \n",
    "          'drugs', 'kids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of 10 observations:\n",
    "sampled_df = essay0_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([32627, 33884, 50509, 48305, 6440, 42908, 7571, 37848], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Checking indexes of dataframe that are included in the sample:\n",
    "print(sampled_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function helps with classification & making sure that tensors are all the\n",
    "# same length:\n",
    "\n",
    "def classify_essay(essay):\n",
    "\n",
    "    # Perform zero-shot classification\n",
    "    output = theme_pipeline(essay, candidate_labels)\n",
    "    \n",
    "    # Create a dictionary mapping labels to scores:\n",
    "    score_dict = {label: score for label, score in zip(\n",
    "        output['labels'], output['scores'])}\n",
    "    \n",
    "    # Ensure all candidate labels have a score, set to 0 if missing:\n",
    "    for label in candidate_labels:\n",
    "        if label not in score_dict:\n",
    "            score_dict[label] = 0.0\n",
    "    \n",
    "    # Convert the dictionary to a pandas Series:\n",
    "    return pd.Series(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying zero-shot classification to sampled dataframe & creating a dataframe:\n",
    "results = pd.concat([sampled_df['essay0'], sampled_df['essay0'].apply(classify_essay)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment for each essay0:\n",
    "def get_sentiment(essay):\n",
    "\n",
    "    # Run sentiment analysis on the essay text\n",
    "    sentiment_result = sentiment_pipeline(essay)\n",
    "\n",
    "    # Extract the sentiment label and score\n",
    "    label = sentiment_result[0]['label']\n",
    "    score = sentiment_result[0]['score']\n",
    "\n",
    "    # Return a dictionary with label and score\n",
    "    return {\n",
    "        'sentiment_label': label, 'sentiment_score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = [get_sentiment(essay) for essay in results.loc[:, 'essay0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything together, so the output contains original biographical data, interest probabilities, and sentiment info\n",
    "df_label_sentiment = pd.concat([\n",
    "    df_sample.loc[:, ['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks', 'drugs',\n",
    "                     'education', 'ethnicity', 'height', 'income', 'job', 'location',\n",
    "                     'offspring', 'pets', 'religion', 'sign', 'smokes', 'speaks']],\n",
    "    results], axis = 1).reset_index()\n",
    "\n",
    "df_label_sentiment = pd.concat([df_label_sentiment, pd.DataFrame(sentiment_dict)], axis = 1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_sentiment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_sentiment.to_csv('classifier_outputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Saving probabilities into a dataset after computing all of them, consult with Ethan on the topic names prior to running everything\n",
    "\n",
    "----\n",
    "\n",
    "Generative AI portion:\n",
    "\n",
    "- import gpt-2 or some other model\n",
    "- fine tune model on the essays in the dataset\n",
    "- generate text in response to the input essay\n",
    "\n",
    "link with example of gpt2: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "- How do we incorporate higher matches to train the model? ## Ask the TA? \n",
    "    - One model is most likely feasible, but the best we have brain stormed is what if we do multiple models (one for each label)\n",
    "        - When user inputs text, classifier identifies top topic -> generate text in response with corresponding topic model\n",
    "\n",
    "-----\n",
    "\n",
    "Matching methodology:\n",
    "\n",
    "- One proposal: lets take the probabilities for each label between two individuals and compute the distance between the probabilities across ALL categories and aggregate them, that is the \"compatiability index\". \n",
    "    - Only conduct matching search for those who are compatiable, sexuality-wise\n",
    "        - Goal is to limit the amount of cross-computation\n",
    "    - Compute compatiability index ONLY between people who have the same top topic AND sexualtiy\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
