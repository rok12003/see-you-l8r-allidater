{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 23:11:20.242825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import statements:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "# Hugging face import:\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "theme_pipeline = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data:\n",
    "df = pd.read_csv(\"/Users/amaribauer/Desktop/A_ML/FinalProject/okcupid_profiles.csv\")\n",
    "#df = pd.read_csv(\"/Users/rohitkandala/Desktop/UChicago/Academic Quarters/2023-24/Winter 2024/CAPP 30255/Project/okcupid_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "location           0\n",
       "last_online        0\n",
       "income             0\n",
       "sex                0\n",
       "status             0\n",
       "orientation        0\n",
       "height             3\n",
       "speaks            50\n",
       "drinks          2985\n",
       "body_type       5296\n",
       "essay0          5488\n",
       "smokes          5512\n",
       "ethnicity       5680\n",
       "education       6628\n",
       "essay1          7572\n",
       "job             8198\n",
       "essay2          9638\n",
       "essay4         10537\n",
       "essay5         10850\n",
       "sign           11056\n",
       "essay3         11476\n",
       "essay7         12451\n",
       "essay9         12603\n",
       "essay6         13771\n",
       "drugs          14080\n",
       "essay8         19225\n",
       "pets           19921\n",
       "religion       20226\n",
       "diet           24395\n",
       "offspring      35561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "essay0- My self summary\n",
    "essay1- What I’m doing with my life\n",
    "essay2- I’m really good at\n",
    "essay3- The first thing people usually notice about me\n",
    "essay4- Favorite books, movies, show, music, and food\n",
    "essay5- The six things I could never do without\n",
    "essay6- I spend a lot of time thinking about\n",
    "essay7- On a typical Friday night I am\n",
    "essay8- The most private thing I am willing to admit\n",
    "essay9- You should message me if..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with just the essays:\n",
    "# I limited us to half the essays for computation reasons. We can expand back if we see the need\n",
    "which_essays = [\"essay0\", \"essay1\", \"essay2\", \"essay3\",  \"essay5\"]\n",
    "essays_df = df.loc[:,which_essays ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaning function that removes stopwords, etc - potentially already done by pretrained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed Additions to labels: travel, drinking, drugs, kids\n",
    "Proposed Removals: teen, enthusiastic, time periods, avid, miscellaneous, rock, sci-fi, favorite, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['TV hits', 'enthusiastic', 'movies', 'music',\n",
    "          'comedies', 'food', 'teen', 'everything', 'drama', 'time periods', \n",
    "          'avid', 'miscellaneous', 'rock', 'music', 'sci-fi', 'favorite', \n",
    "          'novelty', 'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am a chef: this is what that means. 1. i am ...</td>\n",
       "      <td>dedicating everyday to being an unbelievable b...</td>\n",
       "      <td>being silly. having ridiculous amonts of fun w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>delicious porkness in all of its glories. my b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm not ashamed of much, but writing public te...</td>\n",
       "      <td>i make nerdy software for musicians, artists, ...</td>\n",
       "      <td>improvising in different contexts. alternating...</td>\n",
       "      <td>my large jaw and large glasses are the physica...</td>\n",
       "      <td>movement conversation creation contemplation t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i work in a library and go to school. . .</td>\n",
       "      <td>reading things written by old dead people</td>\n",
       "      <td>playing synthesizers and organizing books acco...</td>\n",
       "      <td>socially awkward but i do my best</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey how's it going? currently vague on the pro...</td>\n",
       "      <td>work work work work + play</td>\n",
       "      <td>creating imagery to look at: http://bagsbrown....</td>\n",
       "      <td>i smile a lot and my inquisitive nature</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59941</th>\n",
       "      <td>vibrant, expressive, caring optimist. i love b...</td>\n",
       "      <td>the happiest times have been when life came to...</td>\n",
       "      <td>i make an outstanding osso bucco. i am also ve...</td>\n",
       "      <td>i am told that people notice my smile, eyes an...</td>\n",
       "      <td>my family, my dog, italy, words and music!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59942</th>\n",
       "      <td>i'm nick. i never know what to write about mys...</td>\n",
       "      <td>currently finishing school for film production...</td>\n",
       "      <td>filmmaking, photography, graphic design, web d...</td>\n",
       "      <td>dude, i don't know.</td>\n",
       "      <td>iphone contact lenses headphones camera tv rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59943</th>\n",
       "      <td>hello! i enjoy traveling, watching movies, and...</td>\n",
       "      <td>i'm a civil engineer, who enjoys helping the c...</td>\n",
       "      <td>- looking at things objectively - getting thin...</td>\n",
       "      <td>i'm quiet until i get used to the environment ...</td>\n",
       "      <td>- iphone - friends and family - internet - bay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59944</th>\n",
       "      <td>\"all i have in this world are my balls and my ...</td>\n",
       "      <td>following my dreams... \"you got a dream... you...</td>\n",
       "      <td>listening</td>\n",
       "      <td>it used to be the hair until i mowed it off bu...</td>\n",
       "      <td>music, family, friends, a basketball, hoop, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59945</th>\n",
       "      <td>is it odd that having a little \"enemy\" status ...</td>\n",
       "      <td>i work with elderly people (psychotherapy and ...</td>\n",
       "      <td>i'm a great bullshitter. i don't know what it ...</td>\n",
       "      <td>either that i am funny/sarcastic, or that i am...</td>\n",
       "      <td>1. family &amp; friends &amp; other humans - interacti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59946 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay0  \\\n",
       "0      about me:  i would love to think that i was so...   \n",
       "1      i am a chef: this is what that means. 1. i am ...   \n",
       "2      i'm not ashamed of much, but writing public te...   \n",
       "3              i work in a library and go to school. . .   \n",
       "4      hey how's it going? currently vague on the pro...   \n",
       "...                                                  ...   \n",
       "59941  vibrant, expressive, caring optimist. i love b...   \n",
       "59942  i'm nick. i never know what to write about mys...   \n",
       "59943  hello! i enjoy traveling, watching movies, and...   \n",
       "59944  \"all i have in this world are my balls and my ...   \n",
       "59945  is it odd that having a little \"enemy\" status ...   \n",
       "\n",
       "                                                  essay1  \\\n",
       "0      currently working as an international agent fo...   \n",
       "1      dedicating everyday to being an unbelievable b...   \n",
       "2      i make nerdy software for musicians, artists, ...   \n",
       "3              reading things written by old dead people   \n",
       "4                             work work work work + play   \n",
       "...                                                  ...   \n",
       "59941  the happiest times have been when life came to...   \n",
       "59942  currently finishing school for film production...   \n",
       "59943  i'm a civil engineer, who enjoys helping the c...   \n",
       "59944  following my dreams... \"you got a dream... you...   \n",
       "59945  i work with elderly people (psychotherapy and ...   \n",
       "\n",
       "                                                  essay2  \\\n",
       "0      making people laugh. ranting about a good salt...   \n",
       "1      being silly. having ridiculous amonts of fun w...   \n",
       "2      improvising in different contexts. alternating...   \n",
       "3      playing synthesizers and organizing books acco...   \n",
       "4      creating imagery to look at: http://bagsbrown....   \n",
       "...                                                  ...   \n",
       "59941  i make an outstanding osso bucco. i am also ve...   \n",
       "59942  filmmaking, photography, graphic design, web d...   \n",
       "59943  - looking at things objectively - getting thin...   \n",
       "59944                                          listening   \n",
       "59945  i'm a great bullshitter. i don't know what it ...   \n",
       "\n",
       "                                                  essay3  \\\n",
       "0      the way i look. i am a six foot half asian, ha...   \n",
       "1                                                    NaN   \n",
       "2      my large jaw and large glasses are the physica...   \n",
       "3                      socially awkward but i do my best   \n",
       "4                i smile a lot and my inquisitive nature   \n",
       "...                                                  ...   \n",
       "59941  i am told that people notice my smile, eyes an...   \n",
       "59942                                dude, i don't know.   \n",
       "59943  i'm quiet until i get used to the environment ...   \n",
       "59944  it used to be the hair until i mowed it off bu...   \n",
       "59945  either that i am funny/sarcastic, or that i am...   \n",
       "\n",
       "                                                  essay5  \n",
       "0                      food. water. cell phone. shelter.  \n",
       "1      delicious porkness in all of its glories. my b...  \n",
       "2      movement conversation creation contemplation t...  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "59941         my family, my dog, italy, words and music!  \n",
       "59942  iphone contact lenses headphones camera tv rem...  \n",
       "59943  - iphone - friends and family - internet - bay...  \n",
       "59944  music, family, friends, a basketball, hoop, so...  \n",
       "59945  1. family & friends & other humans - interacti...  \n",
       "\n",
       "[59946 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am applying the thematic classifier to the first row of the dataframe.\n",
    "#This outputs a list of dictionaries with the sequence, labels, and scores keys\n",
    "results = essays_df.iloc[0].apply(lambda essay: theme_pipeline(essay, candidate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"about me:  i would love to think that i was some some kind of intellectual: either the dumbest smart guy, or the smartest dumb guy. can't say i can tell the difference. i love to talk about ideas and concepts. i forge odd metaphors instead of reciting cliches. like the simularities between a friend of mine's house and an underwater salt mine. my favorite word is salt by the way (weird choice i know). to me most things in life are better as metaphors. i seek to make myself a little better everyday, in some productively lazy way. got tired of tying my shoes. considered hiring a five year old, but would probably have to tie both of our shoes... decided to only wear leather shoes dress shoes.  about you:  you love to have really serious, really deep conversations about really silly stuff. you have to be willing to snap me out of a light hearted rant with a kiss. you don't have to be funny, but you have to be able to make me laugh. you should be able to bend spoons with your mind, and telepathically make me smile while i am still at work. you should love life, and be cool with just letting the wind blow. extra points for reading all this and guessing my favorite video game (no hints given yet). and lastly you have a good attention span.\",\n",
       " 'labels': ['favorite',\n",
       "  'enthusiastic',\n",
       "  'avid',\n",
       "  'miscellaneous',\n",
       "  'novelty',\n",
       "  'drama',\n",
       "  'time periods',\n",
       "  'teen',\n",
       "  'everything',\n",
       "  'TV hits',\n",
       "  'music',\n",
       "  'music',\n",
       "  'rock',\n",
       "  'comedies',\n",
       "  'sci-fi',\n",
       "  'food',\n",
       "  'books',\n",
       "  'movies'],\n",
       " 'scores': [0.23194371163845062,\n",
       "  0.1670464128255844,\n",
       "  0.15669363737106323,\n",
       "  0.10210663825273514,\n",
       "  0.10001099854707718,\n",
       "  0.035727359354496,\n",
       "  0.033144447952508926,\n",
       "  0.029779134318232536,\n",
       "  0.025707770138978958,\n",
       "  0.021384108811616898,\n",
       "  0.015098179690539837,\n",
       "  0.015098179690539837,\n",
       "  0.014129006303846836,\n",
       "  0.014098064042627811,\n",
       "  0.011623923666775227,\n",
       "  0.0112119996920228,\n",
       "  0.008332195691764355,\n",
       "  0.006864222697913647]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " \"about me:  i would love to think that i was some some kind of intellectual: either the dumbest smart guy, or the smartest dumb guy. can't say i can tell the difference. i love to talk about ideas and concepts. i forge odd metaphors instead of reciting cliches. like the simularities between a friend of mine's house and an underwater salt mine. my favorite word is salt by the way (weird choice i know). to me most things in life are better as metaphors. i seek to make myself a little better everyday, in some productively lazy way. got tired of tying my shoes. considered hiring a five year old, but would probably have to tie both of our shoes... decided to only wear leather shoes dress shoes.  about you:  you love to have really serious, really deep conversations about really silly stuff. you have to be willing to snap me out of a light hearted rant with a kiss. you don't have to be funny, but you have to be able to make me laugh. you should be able to bend spoons with your mind, and telepathically make me smile while i am still at work. you should love life, and be cool with just letting the wind blow. extra points for reading all this and guessing my favorite video game (no hints given yet). and lastly you have a good attention span.\",\n",
       " 'favorite',\n",
       " [{'label': 'POSITIVE', 'score': 0.9970517158508301}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am applying the sentiment model to each essay in the first row.\n",
    "#For each essay, we'll have a top theme and sentiment result\n",
    "sentiments = pd.DataFrame(results.apply(lambda essay: (results.name, essay['sequence'],essay['labels'][0], sentiment_pipeline(essay['sequence']))))                          \n",
    "sentiments[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: BUILD OUT LOGIC TO PROCESS MORE ROWS\n",
    "\n",
    "# Apply the sentiment_pipeline function to each essay in the results Series and transpose the resulting DataFrame\n",
    "sentiments_df = pd.DataFrame(results.apply(lambda essay: [results.name, \n",
    "        essay['sequence'], essay['labels'][0], sentiment_pipeline(essay['sequence'])]).tolist(\n",
    "        ), columns=['Index', 'Sequence', 'Label', 'Sentiment']).transpose()\n",
    "\n",
    "# Combine all columns into a single row\n",
    "long_col_names =  which_essays + ['theme1', 'theme2','theme3', 'theme4', 'theme5'] + ['sent1','sent2','sent3','sent4','sent5']\n",
    "sentiments_combined = pd.DataFrame(sentiments_df.values.flatten().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Our end result here, sentiments_long, is a pandas df with the essay responses, the top themes, and sentiment analysis response for five essays. Our output for the eventual final classifier would a pandas dataframe with this information for every profile. An alternative implementation could be to include each essay, top theme, and sentiment into a tuple, and then we'd only have five columns per person instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay5</th>\n",
       "      <th>theme1</th>\n",
       "      <th>theme2</th>\n",
       "      <th>theme3</th>\n",
       "      <th>theme4</th>\n",
       "      <th>theme5</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>sent3</th>\n",
       "      <th>sent4</th>\n",
       "      <th>sent5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "      <td>favorite</td>\n",
       "      <td>books</td>\n",
       "      <td>enthusiastic</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>food</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.997051715850...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997330904006...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998056292533...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.853669345378...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.740077495574...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              essay0  \\\n",
       "0  about me:  i would love to think that i was so...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0  currently working as an international agent fo...   \n",
       "\n",
       "                                              essay2  \\\n",
       "0  making people laugh. ranting about a good salt...   \n",
       "\n",
       "                                              essay3  \\\n",
       "0  the way i look. i am a six foot half asian, ha...   \n",
       "\n",
       "                              essay5    theme1 theme2        theme3  \\\n",
       "0  food. water. cell phone. shelter.  favorite  books  enthusiastic   \n",
       "\n",
       "          theme4 theme5                                              sent1  \\\n",
       "0  miscellaneous   food  [{'label': 'POSITIVE', 'score': 0.997051715850...   \n",
       "\n",
       "                                               sent2  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.997330904006...   \n",
       "\n",
       "                                               sent3  \\\n",
       "0  [{'label': 'POSITIVE', 'score': 0.998056292533...   \n",
       "\n",
       "                                               sent4  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.853669345378...   \n",
       "\n",
       "                                               sent5  \n",
       "0  [{'label': 'NEGATIVE', 'score': 0.740077495574...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_long = sentiments_combined.drop(sentiments_combined.columns[:5], axis = 1)\n",
    "sentiments_long.columns = long_col_names\n",
    "sentiments_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Saving probabilities into a dataset after computing all of them, consult with Ethan on the topic names prior to running everything\n",
    "\n",
    "----\n",
    "\n",
    "Generative AI portion:\n",
    "\n",
    "- import gpt-2 or some other model\n",
    "- fine tune model on the essays in the dataset\n",
    "- generate text in response to the input essay\n",
    "\n",
    "link with example of gpt2: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "- How do we incorporate higher matches to train the model? ## Ask the TA? \n",
    "    - One model is most likely feasible, but the best we have brain stormed is what if we do multiple models (one for each label)\n",
    "        - When user inputs text, classifier identifies top topic -> generate text in response with corresponding topic model\n",
    "\n",
    "-----\n",
    "\n",
    "Matching methodology:\n",
    "\n",
    "- One proposal: lets take the probabilities for each label between two individuals and compute the distance between the probabilities across ALL categories and aggregate them, that is the \"compatiability index\". \n",
    "    - Only conduct matching search for those who are compatiable, sexuality-wise\n",
    "        - Goal is to limit the amount of cross-computation\n",
    "    - Compute compatiability index ONLY between people who have the same top topic AND sexualtiy\n",
    "\n",
    "-----\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
