{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Hugging face import:\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66e439e6f7a42e4851c02fc5e5d147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade9057f16f44309b58241af7a3f874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412a54652a104c76ab72acc5039e76d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9759446ed6044b48277aa37d853e3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving zero-shot classification: \n",
    "theme_pipeline = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Saving sentiment pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data:\n",
    "#df = pd.read_csv(\"/Users/amaribauer/Desktop/A_ML/FinalProject/okcupid_profiles.csv\")\n",
    "df = pd.read_csv(\"/Users/rohitkandala/Desktop/UChicago/Academic Quarters/2023-24/Winter 2024/CAPP 30255/Project/okcupid_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essay dataframe:\n",
    "essays_df = df.loc[:, [\"essay0\", \"essay1\", \"essay2\", \"essay3\", \"essay4\", \n",
    "                   \"essay5\", \"essay6\", \"essay7\", \"essay8\", \"essay9\"]]\n",
    "essays_df = essays_df.astype(str)\n",
    "\n",
    "# Essay0 dataframe only \"about me\":\n",
    "essay0_df = df.loc[:, [\"essay0\"]]\n",
    "essay0_df = essay0_df.dropna(subset=['essay0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaning function that removes stopwords, etc - potentially already done by pretrained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed Additions to labels: travel, drinking, drugs, kids\n",
    "Proposed Removals: teen, enthusiastic, time periods, avid, miscellaneous, rock, sci-fi, favorite, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting labels. Think of thse as our classes:\n",
    "candidate_labels = ['TV hits', 'enthusiastic', 'movies', 'music',\n",
    "          'comedies', 'food', 'teen', 'everything', 'drama', 'time periods', \n",
    "          'avid', 'miscellaneous', 'rock', 'music', 'sci-fi', 'favorite', \n",
    "          'novelty', 'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of 10 observations:\n",
    "sampled_df = essay0_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([34118, 18708, 31958, 27741, 29665, 41977, 4789, 26894, 41198, 31154], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Checking indexes of dataframe that are included in the sample:\n",
    "print(sampled_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34118</th>\n",
       "      <td>here are some facts with a high degree of trut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18708</th>\n",
       "      <td>i recently graduated with a masters in account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>i likes colors and patterns. i enjoy spending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27741</th>\n",
       "      <td>the words that best describe me are active, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29665</th>\n",
       "      <td>it's really hard to describe myself in 100 cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay0\n",
       "34118  here are some facts with a high degree of trut...\n",
       "18708  i recently graduated with a masters in account...\n",
       "31958  i likes colors and patterns. i enjoy spending ...\n",
       "27741  the words that best describe me are active, vi...\n",
       "29665  it's really hard to describe myself in 100 cha..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function helps with classification & making sure that tensors are all the\n",
    "# same length:\n",
    "\n",
    "def classify_essay(essay):\n",
    "\n",
    "    # Perform zero-shot classification\n",
    "    output = theme_pipeline(essay, candidate_labels)\n",
    "    \n",
    "    # Create a dictionary mapping labels to scores:\n",
    "    score_dict = {label: score for label, score in zip(\n",
    "        output['labels'], output['scores'])}\n",
    "    \n",
    "    # Ensure all candidate labels have a score, set to 0 if missing:\n",
    "    for label in candidate_labels:\n",
    "        if label not in score_dict:\n",
    "            score_dict[label] = 0.0\n",
    "    \n",
    "    # Convert the dictionary to a pandas Series:\n",
    "    return pd.Series(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying zero-shot classification to sampled dataframe & creating a dataframe:\n",
    "results = sampled_df['essay0'].apply(classify_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enthusiastic</th>\n",
       "      <th>avid</th>\n",
       "      <th>food</th>\n",
       "      <th>music</th>\n",
       "      <th>miscellaneous</th>\n",
       "      <th>novelty</th>\n",
       "      <th>favorite</th>\n",
       "      <th>time periods</th>\n",
       "      <th>teen</th>\n",
       "      <th>rock</th>\n",
       "      <th>drama</th>\n",
       "      <th>everything</th>\n",
       "      <th>sci-fi</th>\n",
       "      <th>TV hits</th>\n",
       "      <th>movies</th>\n",
       "      <th>books</th>\n",
       "      <th>comedies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34118</th>\n",
       "      <td>0.279540</td>\n",
       "      <td>0.154115</td>\n",
       "      <td>0.137217</td>\n",
       "      <td>0.113735</td>\n",
       "      <td>0.062601</td>\n",
       "      <td>0.039644</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.017548</td>\n",
       "      <td>0.013915</td>\n",
       "      <td>0.009514</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18708</th>\n",
       "      <td>0.157826</td>\n",
       "      <td>0.434765</td>\n",
       "      <td>0.022420</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.053393</td>\n",
       "      <td>0.052949</td>\n",
       "      <td>0.028648</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.015866</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.009090</td>\n",
       "      <td>0.041317</td>\n",
       "      <td>0.010528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>0.264436</td>\n",
       "      <td>0.156601</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>0.046317</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.038110</td>\n",
       "      <td>0.016730</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.012433</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.191501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27741</th>\n",
       "      <td>0.470504</td>\n",
       "      <td>0.191120</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.031321</td>\n",
       "      <td>0.053294</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.102768</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.004635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29665</th>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.181510</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.024050</td>\n",
       "      <td>0.179129</td>\n",
       "      <td>0.148447</td>\n",
       "      <td>0.050645</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.061061</td>\n",
       "      <td>0.036226</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>0.016278</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.013225</td>\n",
       "      <td>0.016157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41977</th>\n",
       "      <td>0.057058</td>\n",
       "      <td>0.103458</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>0.071587</td>\n",
       "      <td>0.185273</td>\n",
       "      <td>0.088847</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.070695</td>\n",
       "      <td>0.026750</td>\n",
       "      <td>0.034804</td>\n",
       "      <td>0.037131</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.069078</td>\n",
       "      <td>0.025867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>0.074426</td>\n",
       "      <td>0.179151</td>\n",
       "      <td>0.072553</td>\n",
       "      <td>0.016739</td>\n",
       "      <td>0.052094</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.166758</td>\n",
       "      <td>0.055707</td>\n",
       "      <td>0.040257</td>\n",
       "      <td>0.041743</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.012366</td>\n",
       "      <td>0.021353</td>\n",
       "      <td>0.028182</td>\n",
       "      <td>0.017417</td>\n",
       "      <td>0.026036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26894</th>\n",
       "      <td>0.044932</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.007227</td>\n",
       "      <td>0.335098</td>\n",
       "      <td>0.206065</td>\n",
       "      <td>0.046044</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>0.055244</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.008938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41198</th>\n",
       "      <td>0.306287</td>\n",
       "      <td>0.131433</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.006974</td>\n",
       "      <td>0.135380</td>\n",
       "      <td>0.137794</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.087784</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.047342</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.012019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31154</th>\n",
       "      <td>0.501350</td>\n",
       "      <td>0.366823</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.001480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       enthusiastic      avid      food     music  miscellaneous   novelty  \\\n",
       "34118      0.279540  0.154115  0.137217  0.113735       0.062601  0.039644   \n",
       "18708      0.157826  0.434765  0.022420  0.010858       0.053393  0.052949   \n",
       "31958      0.264436  0.156601  0.060781  0.046317       0.065510  0.038110   \n",
       "27741      0.470504  0.191120  0.010947  0.031321       0.053294  0.021188   \n",
       "29665      0.154645  0.181510  0.006840  0.024050       0.179129  0.148447   \n",
       "41977      0.057058  0.103458  0.021358  0.071587       0.185273  0.088847   \n",
       "4789       0.074426  0.179151  0.072553  0.016739       0.052094  0.129600   \n",
       "26894      0.044932  0.173299  0.007152  0.007227       0.335098  0.206065   \n",
       "41198      0.306287  0.131433  0.003635  0.006974       0.135380  0.137794   \n",
       "31154      0.501350  0.366823  0.001421  0.004004       0.037566  0.018263   \n",
       "\n",
       "       favorite  time periods      teen      rock     drama  everything  \\\n",
       "34118  0.020007      0.017548  0.013915  0.009514  0.009237    0.008404   \n",
       "18708  0.028648      0.058519  0.022478  0.015866  0.018481    0.026516   \n",
       "31958  0.016730      0.057300  0.012433  0.012400  0.005459    0.011714   \n",
       "27741  0.102768      0.018213  0.011227  0.019705  0.003613    0.013623   \n",
       "29665  0.050645      0.029104  0.061061  0.036226  0.010375    0.019723   \n",
       "41977  0.057465      0.070695  0.026750  0.034804  0.037131    0.024514   \n",
       "4789   0.035143      0.166758  0.055707  0.040257  0.041743    0.013736   \n",
       "26894  0.046044      0.038007  0.018977  0.011658  0.055244    0.012508   \n",
       "41198  0.026101      0.087784  0.022274  0.047342  0.014976    0.025141   \n",
       "31154  0.022966      0.004276  0.006174  0.006903  0.003260    0.004134   \n",
       "\n",
       "         sci-fi   TV hits    movies     books  comedies  \n",
       "34118  0.005144  0.004558  0.004208  0.003518  0.003361  \n",
       "18708  0.011439  0.014050  0.009090  0.041317  0.010528  \n",
       "31958  0.004129  0.004217  0.002705  0.003341  0.191501  \n",
       "27741  0.004863  0.006505  0.002312  0.002842  0.004635  \n",
       "29665  0.016278  0.018012  0.010524  0.013225  0.016157  \n",
       "41977  0.011995  0.026255  0.016279  0.069078  0.025867  \n",
       "4789   0.012366  0.021353  0.028182  0.017417  0.026036  \n",
       "26894  0.011739  0.005333  0.004653  0.005898  0.008938  \n",
       "41198  0.010097  0.012470  0.007432  0.005886  0.012019  \n",
       "31154  0.005597  0.002447  0.003841  0.005490  0.001480  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing dataframe:\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment for each essay0:\n",
    "def get_sentiment(essay):\n",
    "\n",
    "    # Run sentiment analysis on the essay text\n",
    "    sentiment_result = sentiment_pipeline(essay)\n",
    "\n",
    "    # Extract the sentiment label and score\n",
    "    label = sentiment_result[0]['label']\n",
    "    score = sentiment_result[0]['score']\n",
    "\n",
    "    # Return a dictionary with label and score\n",
    "    return {'sentiment_label': label, 'sentiment_score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: BUILD OUT LOGIC TO PROCESS MORE ROWS\n",
    "\n",
    "# Apply the sentiment_pipeline function to each essay in the results Series and transpose the resulting DataFrame\n",
    "sentiments_df = pd.DataFrame(results.apply(lambda essay: [results.name, \n",
    "        essay['sequence'], essay['labels'][0], sentiment_pipeline(essay['sequence'])]).tolist(\n",
    "        ), columns=['Index', 'Sequence', 'Label', 'Sentiment']).transpose()\n",
    "\n",
    "# Combine all columns into a single row\n",
    "long_col_names =  which_essays + ['theme1', 'theme2','theme3', 'theme4', 'theme5'] + ['sent1','sent2','sent3','sent4','sent5']\n",
    "sentiments_combined = pd.DataFrame(sentiments_df.values.flatten().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Our end result here, sentiments_long, is a pandas df with the essay responses, the top themes, and sentiment analysis response for five essays. Our output for the eventual final classifier would a pandas dataframe with this information for every profile. An alternative implementation could be to include each essay, top theme, and sentiment into a tuple, and then we'd only have five columns per person instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay5</th>\n",
       "      <th>theme1</th>\n",
       "      <th>theme2</th>\n",
       "      <th>theme3</th>\n",
       "      <th>theme4</th>\n",
       "      <th>theme5</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>sent3</th>\n",
       "      <th>sent4</th>\n",
       "      <th>sent5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "      <td>favorite</td>\n",
       "      <td>books</td>\n",
       "      <td>enthusiastic</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>food</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.997051715850...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997330904006...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998056292533...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.853669345378...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.740077495574...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              essay0  \\\n",
       "0  about me:  i would love to think that i was so...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0  currently working as an international agent fo...   \n",
       "\n",
       "                                              essay2  \\\n",
       "0  making people laugh. ranting about a good salt...   \n",
       "\n",
       "                                              essay3  \\\n",
       "0  the way i look. i am a six foot half asian, ha...   \n",
       "\n",
       "                              essay5    theme1 theme2        theme3  \\\n",
       "0  food. water. cell phone. shelter.  favorite  books  enthusiastic   \n",
       "\n",
       "          theme4 theme5                                              sent1  \\\n",
       "0  miscellaneous   food  [{'label': 'POSITIVE', 'score': 0.997051715850...   \n",
       "\n",
       "                                               sent2  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.997330904006...   \n",
       "\n",
       "                                               sent3  \\\n",
       "0  [{'label': 'POSITIVE', 'score': 0.998056292533...   \n",
       "\n",
       "                                               sent4  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.853669345378...   \n",
       "\n",
       "                                               sent5  \n",
       "0  [{'label': 'NEGATIVE', 'score': 0.740077495574...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_long = sentiments_combined.drop(sentiments_combined.columns[:5], axis = 1)\n",
    "sentiments_long.columns = long_col_names\n",
    "sentiments_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Saving probabilities into a dataset after computing all of them, consult with Ethan on the topic names prior to running everything\n",
    "\n",
    "----\n",
    "\n",
    "Generative AI portion:\n",
    "\n",
    "- import gpt-2 or some other model\n",
    "- fine tune model on the essays in the dataset\n",
    "- generate text in response to the input essay\n",
    "\n",
    "link with example of gpt2: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "- How do we incorporate higher matches to train the model? ## Ask the TA? \n",
    "    - One model is most likely feasible, but the best we have brain stormed is what if we do multiple models (one for each label)\n",
    "        - When user inputs text, classifier identifies top topic -> generate text in response with corresponding topic model\n",
    "\n",
    "-----\n",
    "\n",
    "Matching methodology:\n",
    "\n",
    "- One proposal: lets take the probabilities for each label between two individuals and compute the distance between the probabilities across ALL categories and aggregate them, that is the \"compatiability index\". \n",
    "    - Only conduct matching search for those who are compatiable, sexuality-wise\n",
    "        - Goal is to limit the amount of cross-computation\n",
    "    - Compute compatiability index ONLY between people who have the same top topic AND sexualtiy\n",
    "\n",
    "-----\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
