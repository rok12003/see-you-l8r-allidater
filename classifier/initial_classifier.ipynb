{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:33:42.877522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import statements:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Hugging face import:\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving zero-shot classification: \n",
    "theme_pipeline = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Saving sentiment pipeline\n",
    "# Ethan: specifying a model to ensure pipeline stability as per Huggingface recommendation\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data:\n",
    "df = pd.read_csv(\"/Users/amaribauer/Desktop/A_ML/FinalProject/okcupid_profiles.csv\")\n",
    "# Ethan: using a relative path for reproducability. To reproduce, add a data file in your home directory and put the profile document there.\n",
    "#df = pd.read_csv(\"../data/okcupid_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethan: sampling here rather than after data cleaning\n",
    "df_sample = df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essay dataframe:\n",
    "essays_df = df_sample.loc[:, [\"essay0\", \"essay1\", \"essay2\", \"essay3\", \"essay4\", \n",
    "                   \"essay5\", \"essay6\", \"essay7\", \"essay8\", \"essay9\"]]\n",
    "essays_df = essays_df.astype(str)\n",
    "\n",
    "# Essay0 dataframe only \"about me\":\n",
    "essay0_df = df_sample.loc[:, [\"essay0\"]]\n",
    "essay0_df = essay0_df.dropna(subset=['essay0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed Additions to labels: travel, drinking, drugs, kids\n",
    "Proposed Removals: teen, enthusiastic, time periods, avid, miscellaneous, rock, sci-fi, favorite, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting labels. Think of thse as our classes:\n",
    "# Ethan: Implemented Amari's suggestion for label names. We can tweak this more going forward.\n",
    "candidate_labels = ['TV', 'movies', 'music',\n",
    "          'comedies', 'food', 'drama',\n",
    "          'music', 'books', 'travel', 'drinking', \n",
    "          'drugs', 'kids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of 10 observations:\n",
    "sampled_df = essay0_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([32627, 33884, 50509, 48305, 6440, 42908, 7571, 37848], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Checking indexes of dataframe that are included in the sample:\n",
    "print(sampled_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function helps with classification & making sure that tensors are all the\n",
    "# same length:\n",
    "\n",
    "def classify_essay(essay):\n",
    "    \"\"\"\n",
    "    Classifies essay using theme classification. \n",
    "    \"\"\"\n",
    "\n",
    "    # Perform zero-shot classification\n",
    "    output = theme_pipeline(essay, candidate_labels)\n",
    "    \n",
    "    # Create a dictionary mapping labels to scores:\n",
    "    score_dict = {label: score for label, score in zip(\n",
    "        output['labels'], output['scores'])}\n",
    "    \n",
    "    # Ensure all candidate labels have a score, set to 0 if missing:\n",
    "    for label in candidate_labels:\n",
    "        if label not in score_dict:\n",
    "            score_dict[label] = 0.0\n",
    "    \n",
    "    # Convert the dictionary to a pandas Series:\n",
    "    return pd.Series(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying zero-shot classification to sampled dataframe & creating a dataframe:\n",
    "results = pd.concat([sampled_df['essay0'], sampled_df['essay0'].apply(classify_essay)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>food</th>\n",
       "      <th>kids</th>\n",
       "      <th>travel</th>\n",
       "      <th>drama</th>\n",
       "      <th>music</th>\n",
       "      <th>TV</th>\n",
       "      <th>comedies</th>\n",
       "      <th>movies</th>\n",
       "      <th>drinking</th>\n",
       "      <th>books</th>\n",
       "      <th>drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32627</th>\n",
       "      <td>i enjoy being akward on purpose. i love my bik...</td>\n",
       "      <td>0.529102</td>\n",
       "      <td>0.076941</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.063269</td>\n",
       "      <td>0.042128</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.017707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33884</th>\n",
       "      <td>wow... i appreciate pauses, and i also appreci...</td>\n",
       "      <td>0.200013</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>0.467908</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.074945</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>0.005736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50509</th>\n",
       "      <td>\"life of the party\" is an understatement. i am...</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>0.056041</td>\n",
       "      <td>0.178071</td>\n",
       "      <td>0.076248</td>\n",
       "      <td>0.136479</td>\n",
       "      <td>0.087189</td>\n",
       "      <td>0.103010</td>\n",
       "      <td>0.046710</td>\n",
       "      <td>0.081643</td>\n",
       "      <td>0.037309</td>\n",
       "      <td>0.022735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48305</th>\n",
       "      <td>intellectual and slightly wacky fellow here. i...</td>\n",
       "      <td>0.036455</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.201182</td>\n",
       "      <td>0.168858</td>\n",
       "      <td>0.069260</td>\n",
       "      <td>0.089926</td>\n",
       "      <td>0.074984</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>0.036615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6440</th>\n",
       "      <td>i love what i do and where i'm at in life. alt...</td>\n",
       "      <td>0.043237</td>\n",
       "      <td>0.129924</td>\n",
       "      <td>0.120587</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.163226</td>\n",
       "      <td>0.079852</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.111011</td>\n",
       "      <td>0.030546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42908</th>\n",
       "      <td>i keep lots of pens and paper around. i enjoy ...</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.112259</td>\n",
       "      <td>0.393783</td>\n",
       "      <td>0.221974</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.036303</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>0.018199</td>\n",
       "      <td>0.059223</td>\n",
       "      <td>0.022158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>i'm open minded, i enjoy learning and learning...</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.661747</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.155575</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37848</th>\n",
       "      <td>the quintessential san francisco experience is...</td>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.040261</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.069344</td>\n",
       "      <td>0.109557</td>\n",
       "      <td>0.029009</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.007743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay0      food      kids  \\\n",
       "32627  i enjoy being akward on purpose. i love my bik...  0.529102  0.076941   \n",
       "33884  wow... i appreciate pauses, and i also appreci...  0.200013  0.023789   \n",
       "50509  \"life of the party\" is an understatement. i am...  0.038085  0.056041   \n",
       "48305  intellectual and slightly wacky fellow here. i...  0.036455  0.054719   \n",
       "6440   i love what i do and where i'm at in life. alt...  0.043237  0.129924   \n",
       "42908  i keep lots of pens and paper around. i enjoy ...  0.020918  0.112259   \n",
       "7571   i'm open minded, i enjoy learning and learning...  0.002703  0.006868   \n",
       "37848  the quintessential san francisco experience is...  0.016285  0.018308   \n",
       "\n",
       "         travel     drama     music        TV  comedies    movies  drinking  \\\n",
       "32627  0.073700  0.063269  0.042128  0.039525  0.037574  0.029011  0.025359   \n",
       "33884  0.467908  0.016887  0.074945  0.012192  0.050511  0.013237  0.027108   \n",
       "50509  0.178071  0.076248  0.136479  0.087189  0.103010  0.046710  0.081643   \n",
       "48305  0.201182  0.168858  0.069260  0.089926  0.074984  0.047286  0.029727   \n",
       "6440   0.120587  0.072007  0.072581  0.163226  0.079852  0.057335  0.047112   \n",
       "42908  0.393783  0.221974  0.032200  0.031314  0.036303  0.019470  0.018199   \n",
       "7571   0.661747  0.003763  0.155575  0.003796  0.002200  0.001876  0.001531   \n",
       "37848  0.040261  0.027033  0.329929  0.013222  0.069344  0.109557  0.029009   \n",
       "\n",
       "          books     drugs  \n",
       "32627  0.023555  0.017707  \n",
       "33884  0.032730  0.005736  \n",
       "50509  0.037309  0.022735  \n",
       "48305  0.121727  0.036615  \n",
       "6440   0.111011  0.030546  \n",
       "42908  0.059223  0.022158  \n",
       "7571   0.003603  0.000765  \n",
       "37848  0.009380  0.007743  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment for each essay0:\n",
    "def get_sentiment(essay):\n",
    "    \"\"\"\n",
    "    Returns sentiment on entire essay corpus/observation.  \n",
    "    \"\"\"\n",
    "\n",
    "   # Run sentiment analysis on the essay text\n",
    "    sentiment_result = sentiment_pipeline(essay)\n",
    "\n",
    "    # Extract the sentiment label and score\n",
    "    label = sentiment_result[0]['label']\n",
    "    score = sentiment_result[0]['score']\n",
    "\n",
    "    # Return a dictionary with label and score\n",
    "    return {\n",
    "        'sentiment_label': label, 'sentiment_score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = [get_sentiment(essay) for essay in results.loc[:, 'essay0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks',\n",
       "       'drugs', 'education', 'ethnicity', 'height', 'income', 'job',\n",
       "       'last_online', 'location', 'offspring', 'pets', 'religion', 'sign',\n",
       "       'smokes', 'speaks', 'essay0', 'essay1', 'essay2', 'essay3', 'essay4',\n",
       "       'essay5', 'essay6', 'essay7', 'essay8', 'essay9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything together, so the output contains original biographical data, interest probabilities, and sentiment info\n",
    "df_label_sentiment = pd.concat([\n",
    "    df_sample.loc[:, ['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks', 'drugs',\n",
    "                     'education', 'ethnicity', 'height', 'income', 'job', 'location',\n",
    "                     'offspring', 'pets', 'religion', 'sign', 'smokes', 'speaks']],\n",
    "    results], axis = 1).reset_index()\n",
    "\n",
    "df_label_sentiment = pd.concat([df_label_sentiment, pd.DataFrame(sentiment_dict)], axis = 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'age', 'status', 'sex', 'orientation', 'body_type', 'diet',\n",
       "       'drinks', 'drugs', 'education', 'ethnicity', 'height', 'income', 'job',\n",
       "       'location', 'offspring', 'pets', 'religion', 'sign', 'smokes', 'speaks',\n",
       "       'essay0', 'food', 'kids', 'travel', 'drama', 'music', 'TV', 'comedies',\n",
       "       'movies', 'drinking', 'books', 'drugs', 'sentiment_label',\n",
       "       'sentiment_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_sentiment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_sentiment.to_csv('classifier_outputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>...</th>\n",
       "      <th>drama</th>\n",
       "      <th>music</th>\n",
       "      <th>TV</th>\n",
       "      <th>comedies</th>\n",
       "      <th>movies</th>\n",
       "      <th>drinking</th>\n",
       "      <th>books</th>\n",
       "      <th>drugs</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32627</td>\n",
       "      <td>25</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>fit</td>\n",
       "      <td>anything</td>\n",
       "      <td>very often</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063269</td>\n",
       "      <td>0.042128</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.948390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33884</td>\n",
       "      <td>34</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.074945</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.997833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50509</td>\n",
       "      <td>26</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>mostly other</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from two-year college</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076248</td>\n",
       "      <td>0.136479</td>\n",
       "      <td>0.087189</td>\n",
       "      <td>0.103010</td>\n",
       "      <td>0.046710</td>\n",
       "      <td>0.081643</td>\n",
       "      <td>0.037309</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.983977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48305</td>\n",
       "      <td>41</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>gay</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from ph.d program</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168858</td>\n",
       "      <td>0.069260</td>\n",
       "      <td>0.089926</td>\n",
       "      <td>0.074984</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>0.036615</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.992873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28049</td>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>often</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6440</td>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>f</td>\n",
       "      <td>straight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>rarely</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.163226</td>\n",
       "      <td>0.079852</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.111011</td>\n",
       "      <td>0.030546</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.995419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42908</td>\n",
       "      <td>34</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>graduated from ph.d program</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221974</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.036303</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>0.018199</td>\n",
       "      <td>0.059223</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7571</td>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.155575</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.997934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37848</td>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.069344</td>\n",
       "      <td>0.109557</td>\n",
       "      <td>0.029009</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49365</td>\n",
       "      <td>32</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  age  status sex orientation       body_type               diet  \\\n",
       "0  32627   25  single   m    straight             fit           anything   \n",
       "1  33884   34  single   m    straight         average    mostly anything   \n",
       "2  50509   26  single   m    straight        athletic       mostly other   \n",
       "3  48305   41  single   m         gay         average                NaN   \n",
       "4  28049   29  single   m    straight        athletic    mostly anything   \n",
       "5   6440   29  single   f    straight             NaN  strictly anything   \n",
       "6  42908   34  single   m    straight        athletic    mostly anything   \n",
       "7   7571   29  single   m    straight  a little extra           anything   \n",
       "8  37848   29  single   m    straight        athletic                NaN   \n",
       "9  49365   32  single   m    straight         average    mostly anything   \n",
       "\n",
       "       drinks      drugs                          education  ...     drama  \\\n",
       "0  very often      never                                NaN  ...  0.063269   \n",
       "1    socially  sometimes     graduated from masters program  ...  0.016887   \n",
       "2    socially      never    graduated from two-year college  ...  0.076248   \n",
       "3    socially        NaN        graduated from ph.d program  ...  0.168858   \n",
       "4       often        NaN     graduated from masters program  ...       NaN   \n",
       "5      rarely        NaN  graduated from college/university  ...  0.072007   \n",
       "6    socially  sometimes        graduated from ph.d program  ...  0.221974   \n",
       "7    socially        NaN  graduated from college/university  ...  0.003763   \n",
       "8    socially      never     graduated from masters program  ...  0.027033   \n",
       "9    socially      never     graduated from masters program  ...       NaN   \n",
       "\n",
       "      music        TV  comedies    movies  drinking     books     drugs  \\\n",
       "0  0.042128  0.039525  0.037574  0.029011  0.025359  0.023555  0.017707   \n",
       "1  0.074945  0.012192  0.050511  0.013237  0.027108  0.032730  0.005736   \n",
       "2  0.136479  0.087189  0.103010  0.046710  0.081643  0.037309  0.022735   \n",
       "3  0.069260  0.089926  0.074984  0.047286  0.029727  0.121727  0.036615   \n",
       "4       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "5  0.072581  0.163226  0.079852  0.057335  0.047112  0.111011  0.030546   \n",
       "6  0.032200  0.031314  0.036303  0.019470  0.018199  0.059223  0.022158   \n",
       "7  0.155575  0.003796  0.002200  0.001876  0.001531  0.003603  0.000765   \n",
       "8  0.329929  0.013222  0.069344  0.109557  0.029009  0.009380  0.007743   \n",
       "9       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "  sentiment_label sentiment_score  \n",
       "0        POSITIVE        0.948390  \n",
       "1        POSITIVE        0.997833  \n",
       "2        POSITIVE        0.983977  \n",
       "3        POSITIVE        0.992873  \n",
       "4        POSITIVE        0.999867  \n",
       "5        NEGATIVE        0.995419  \n",
       "6        POSITIVE        0.999723  \n",
       "7        POSITIVE        0.997934  \n",
       "8             NaN             NaN  \n",
       "9             NaN             NaN  \n",
       "\n",
       "[10 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Saving probabilities into a dataset after computing all of them, consult with Ethan on the topic names prior to running everything\n",
    "\n",
    "----\n",
    "\n",
    "Generative AI portion:\n",
    "\n",
    "- import gpt-2 or some other model\n",
    "- fine tune model on the essays in the dataset\n",
    "- generate text in response to the input essay\n",
    "\n",
    "link with example of gpt2: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "- How do we incorporate higher matches to train the model? ## Ask the TA? \n",
    "    - One model is most likely feasible, but the best we have brain stormed is what if we do multiple models (one for each label)\n",
    "        - When user inputs text, classifier identifies top topic -> generate text in response with corresponding topic model\n",
    "\n",
    "-----\n",
    "\n",
    "Matching methodology:\n",
    "\n",
    "- One proposal: lets take the probabilities for each label between two individuals and compute the distance between the probabilities across ALL categories and aggregate them, that is the \"compatiability index\". \n",
    "    - Only conduct matching search for those who are compatiable, sexuality-wise\n",
    "        - Goal is to limit the amount of cross-computation\n",
    "    - Compute compatiability index ONLY between people who have the same top topic AND sexualtiy\n",
    "\n",
    "-----\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
