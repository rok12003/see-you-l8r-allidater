{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/okcupid_profiles.csv\")\n",
    "#df_sample = df.sample(1000).reset_index(drop = True)\n",
    "df_sample = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks',\n",
       "       'drugs', 'education', 'ethnicity', 'height', 'income', 'job',\n",
       "       'last_online', 'location', 'offspring', 'pets', 'religion', 'sign',\n",
       "       'smokes', 'speaks', 'essay0', 'essay1', 'essay2', 'essay3', 'essay4',\n",
       "       'essay5', 'essay6', 'essay7', 'essay8', 'essay9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All essays but essay9, which is \"you should message me if...\"\n",
    "\n",
    "df_demographics = df_sample[df_sample.columns.drop(list(df_sample.filter(regex=\"essay\")))]\n",
    "essays_df = df_sample.loc[:, [\"essay0\", \"essay1\", \"essay2\", \"essay3\", \"essay4\", \n",
    "                   \"essay5\", \"essay6\", \"essay7\", \"essay8\"]]\n",
    "essays_df = essays_df.fillna(\" \").astype(str)\n",
    "\n",
    "essays_df.loc[:, \"all_essays\"] = essays_df.apply(\" \".join, axis = 1)\n",
    "\n",
    "df_all = pd.concat([df_demographics, essays_df.loc[:, [\"all_essays\"]]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:  i would love to think that i was some some kind of intellectual: either the dumbest smart guy, or the smartest dumb guy. can't say i can tell the difference. i love to talk about ideas and concepts. i forge odd metaphors instead of reciting cliches. like the simularities between a friend of mine's house and an underwater salt mine. my favorite word is salt by the way (weird choice i know). to me most things in life are better as metaphors. i seek to make myself a little better everyday, in some productively lazy way. got tired of tying my shoes. considered hiring a five year old, but would probably have to tie both of our shoes... decided to only wear leather shoes dress shoes.  about you:  you love to have really serious, really deep conversations about really silly stuff. you have to be willing to snap me out of a light hearted rant with a kiss. you don't have to be funny, but you have to be able to make me laugh. you should be able to bend spoons with your mind, and telepathically make me smile while i am still at work. you should love life, and be cool with just letting the wind blow. extra points for reading all this and guessing my favorite video game (no hints given yet). and lastly you have a good attention span. currently working as an international agent for a freight forwarding company. import, export, domestic you know the works. online classes and trying to better myself in my free time. perhaps a hours worth of a good book or a video game on a lazy sunday. making people laugh. ranting about a good salting. finding simplicity in complexity, and complexity in simplicity. the way i look. i am a six foot half asian, half caucasian mutt. it makes it tough not to notice me, and for me to blend in. books: absurdistan, the republic, of mice and men (only book that made me want to cry), catcher in the rye, the prince.  movies: gladiator, operation valkyrie, the producers, down periscope.  shows: the borgia, arrested development, game of thrones, monty python  music: aesop rock, hail mary mallon, george thorogood and the delaware destroyers, felt  food: i'm down for anything. food. water. cell phone. shelter. duality and humorous things trying to find someone to hang out with. i am down for anything except a club. i am new to california and looking for someone to wisper my secrets to.\n",
      "NEW BIO\n",
      "i am a chef: this is what that means. 1. i am a workaholic. 2. i love to cook regardless of whether i am at work. 3. i love to drink and eat foods that are probably really bad for me. 4. i love being around people that resemble line 1-3. i love the outdoors and i am an avid skier. if its snowing i will be in tahoe at the very least. i am a very confident and friendly. i'm not interested in acting or being a typical guy. i have no time or patience for rediculous acts of territorial pissing. overall i am a very likable easygoing individual. i am very adventurous and always looking forward to doing new things and hopefully sharing it with the right person. dedicating everyday to being an unbelievable badass. being silly. having ridiculous amonts of fun wherever. being a smart ass. ohh and i can cook. ;)   i am die hard christopher moore fan. i don't really watch a lot of tv unless there is humor involved. i am kind of stuck on 90's alternative music. i am pretty much a fan of everything though... i do need to draw a line at most types of electronica. delicious porkness in all of its glories. my big ass doughboy's sinking into 15 new inches. my overly resilient liver. a good sharp knife. my ps3... it plays blurays too. ;) my over the top energy and my outlook on life... just give me a bag of lemons and see what happens. ;)     i am very open and will share just about anything.\n",
      "NEW BIO\n",
      "i'm not ashamed of much, but writing public text on an online dating site makes me pleasantly uncomfortable. i'll try to be as earnest as possible in the noble endeavor of standing naked before the world.  i've lived in san francisco for 15 years, and both love it and find myself frustrated with its deficits. lots of great friends and acquaintances (which increases my apprehension to put anything on this site), but i'm feeling like meeting some new people that aren't just friends of friends. it's okay if you are a friend of a friend too. chances are, if you make it through the complex filtering process of multiple choice questions, lifestyle statistics, photo scanning, and these indulgent blurbs of text without moving quickly on to another search result, you are probably already a cultural peer and at most 2 people removed. at first, i thought i should say as little as possible here to avoid you, but that seems silly.  as far as culture goes, i'm definitely more on the weird side of the spectrum, but i don't exactly wear it on my sleeve. once you get me talking, it will probably become increasingly apparent that while i'd like to think of myself as just like everybody else (and by some definition i certainly am), most people don't see me that way. that's fine with me. most of the people i find myself gravitating towards are pretty weird themselves. you probably are too. i make nerdy software for musicians, artists, and experimenters to indulge in their own weirdness, but i like to spend time away from the computer when working on my artwork (which is typically more concerned with group dynamics and communication, than with visual form, objects, or technology). i also record and deejay dance, noise, pop, and experimental music (most of which electronic or at least studio based). besides these relatively ego driven activities, i've been enjoying things like meditation and tai chi to try and gently flirt with ego death. improvising in different contexts. alternating between being present and decidedly outside of a moment, or trying to hold both at once. rambling intellectual conversations that hold said conversations in contempt while seeking to find something that transcends them. being critical while remaining generous. listening to and using body language--often performed in caricature or large gestures, if not outright interpretive dance. dry, dark, and raunchy humor. my large jaw and large glasses are the physical things people comment on the most. when sufficiently stimulated, i have an unmistakable cackle of a laugh. after that, it goes in more directions than i care to describe right now. maybe i'll come back to this. okay this is where the cultural matrix gets so specific, it's like being in the crosshairs.  for what it's worth, i find myself reading more non-fiction than fiction. it's usually some kind of philosophy, art, or science text by silly authors such as ranciere, de certeau, bataille, baudrillard, butler, stein, arendt, nietzche, zizek, etc. i'll often throw in some weird new age or pop-psychology book in the mix as well. as for fiction, i enjoy what little i've read of eco, perec, wallace, bolao, dick, vonnegut, atwood, delilo, etc. when i was young, i was a rabid asimov reader.  directors i find myself drawn to are makavejev, kuchar, jodorowsky, herzog, hara, klein, waters, verhoeven, ackerman, hitchcock, lang, gorin, goddard, miike, ohbayashi, tarkovsky, sokurov, warhol, etc. but i also like a good amount of \"trashy\" stuff. too much to name.  i definitely enjoy the character development that happens in long form episodic television over the course of 10-100 episodes, which a 1-2hr movie usually can't compete with. some of my recent tv favorites are: breaking bad, the wire, dexter, true blood, the prisoner, lost, fringe.  a smattered sampling of the vast field of music i like and deejay: art ensemble, sun ra, evan parker, lil wayne, dj funk, mr. fingers, maurizio, rob hood, dan bell, james blake, nonesuch recordings, omar souleyman, ethiopiques, fela kuti, john cage, meredith monk, robert ashley, terry riley, yoko ono, merzbow, tom tom club, jit, juke, bounce, hyphy, snap, crunk, b'more, kuduro, pop, noise, jazz, techno, house, acid, new/no wave, (post)punk, etc.  a few of the famous art/dance/theater folk that might locate my sensibility: andy warhol, bruce nauman, yayoi kusama, louise bourgeois, tino sehgal, george kuchar, michel duchamp, marina abramovic, gelatin, carolee schneeman, gustav metzger, mike kelly, mike smith, andrea fraser, gordon matta-clark, jerzy grotowski, samuel beckett, antonin artaud, tadeusz kantor, anna halperin, merce cunningham, etc. i'm clearly leaving out a younger generation of contemporary artists, many of whom are friends.  local food regulars: sushi zone, chow, ppq, pagolac, lers ros, burma superstar, minako, shalimar, delfina pizza, rosamunde, arinells, suppenkuche, cha-ya, blue plate, golden era, etc. movement conversation creation contemplation touch humor   viewing. listening. dancing. talking. drinking. performing. when i was five years old, i was known as \"the boogerman\".\n",
      "NEW BIO\n",
      "i work in a library and go to school. . . reading things written by old dead people playing synthesizers and organizing books according to the library of congress classification system socially awkward but i do my best bataille, celine, beckett. . . lynch, jarmusch, r.w. fassbender. . . twin peaks & fishing w/ john joy division, throbbing gristle, cabaret voltaire. . . vegetarian pho and coffee   cats and german philosophy    \n",
      "NEW BIO\n",
      "hey how's it going? currently vague on the profile i know, more to come soon. looking to meet new folks outside of my circle of friends. i'm pretty responsive on the reply tip, feel free to drop a line. cheers. work work work work + play creating imagery to look at: http://bagsbrown.blogspot.com/ http://stayruly.blogspot.com/ i smile a lot and my inquisitive nature music: bands, rappers, musicians at the moment: thee oh sees. forever: wu-tang books: artbooks for days audiobooks: my collection, thick (thanks audible) shows: live ones food: with stellar friends whenever movies > tv podcast: radiolab, this american life, the moth, joe rogan, the champs        \n",
      "NEW BIO\n",
      "i'm an australian living in san francisco, but don't hold that against me. i spend most of my days trying to build cool stuff for my company. i speak mandarin and have been known to bust out chinese songs at karaoke. i'm pretty cheeky. someone asked me if that meant something about my arse, which i find really funny.  i'm a little oddball. i have a wild imagination; i like to think of the most improbable reasons people are doing things just for fun. i love to laugh and look for reasons to do so. occasionally this gets me in trouble because people think i'm laughing at them. sometimes i am, but more often i'm only laughing at myself.  i'm an entrepreneur (like everyone else in sf, it seems) and i love what i do. i enjoy parties and downtime in equal measure. intelligence really turns me on and i love people who can teach me new things. building awesome stuff. figuring out what's important. having adventures. looking for treasure. imagining random shit. laughing at aforementioned random shit. being goofy. articulating what i think and feel. convincing people i'm right. admitting when i'm wrong.  i'm also pretty good at helping people think through problems; my friends say i give good advice. and when i don't have a clue how to help, i will say: i give pretty good hug. i have a big smile. i also get asked if i'm wearing blue-coloured contacts (no). books: to kill a mockingbird, lord of the rings, 1984, the farseer trilogy.  music: the beatles, frank sinatra, john mayer, jason mraz, deadmau5, andrew bayer, everything on anjunadeep records, bach, satie.  tv shows: how i met your mother, scrubs, the west wing, breaking bad.  movies: star wars, the godfather pt ii, 500 days of summer, napoleon dynamite, american beauty, lotr  food: thai, vietnamese, shanghai dumplings, pizza! like everyone else, i love my friends and family, and need hugs, human contact, water and sunshine. let's take that as given.  1. something to build 2. something to sing 3. something to play on (my guitar would be first choice) 4. something to write/draw on 5. a big goal worth dreaming about 6. something to laugh at what my contribution to the world is going to be and/or should be. and what's for breakfast. i love breakfast. out with my friends! i cried on my first day at school because a bird shat on my head. true story.\n",
      "NEW BIO\n",
      "life is about the little things. i love to laugh. it's easy to do when one can find beauty and humor in the ugly. this perspective makes for a more gratifying life. it's a gift. we are here to play. digging up buried treasure frolicking witty banter using my camera to extract sums of a whole and share my perspective with the world in hopes of opening up theirs being amused by things most people would miss i am the last unicorn i like books. ones with pictures. reading them is great too. where do people find the time? i spend more time with other people not reading. i collect books. they sit neatly on my bookshelves.  movies are great. especially on movie night. with brownies.  music. i love (love) it all. unless it's country.  i love food. laughter amazing people in my life color curiosity music and rhythm a good pair of sunglasses synchronicity  there is this whole other realm where the fabrics of our life stories intersect as they dance and play in a magical burst of energy. this realm doesn't need you to believe in it in order to maintain. it is a cluster of synchronicities and happenings. it is a gift to those who notice them. something to be treasured appreciated. there is something special in each and every moment that you experience in your daily waking life. this something brings us back to the age old question: if a tree falls in the forest and no one is there to hear it, does it make a sound? this works in the same way. if you are not consciously there to hear it, see it, taste it, smell it, feel it none of this matters, it's still there. pay attention to the little things, those that are often overlooked. see if you can find the magic in this gift we call life. plotting to take over the world with my army of segway riding pandas and fire breathing kittens my typical friday night\n",
      "NEW BIO\n",
      "  writing. meeting new people, spending time with friends, seeing films, going to literary events and lectures, sifting through bookstores and thrift stores, exploring the city. i also work full time at an interactive agency. remembering people's birthdays, sending cards, being thoughtful, arm wrestling i'm rather approachable (a byproduct of being from a small town in the midwest). i like: alphabetized lists, aquariums, autobiographies, beer on tap, ben folds, biking, brunch, citrus, cocktails, color, comfort food, craft projects, dancing, design, diy, essays, fabric stores, field trips, flea markets, foreign films, glee, good, hammond organs, helping lost tourists, indie rock, ice cream, languages, lectures, letterpress, libraries, literary fiction, live shows, mad men, martha stewart living, memoir, mix tapes, non-fiction, npr, plants, puns, sewing, short stories, siestas, singer-songwriters, spicy food, stationery, storytelling, sufjan stevens, talking to strangers, tea, tegan and sara, the office, 30 rock, travel, quilts, quirky movies, wes anderson, wine, writing, yoga. friends, family, notebook/pen, books, music, travel things that amuse and inspire me out and about or relaxing at home with a good book or netflix  \n",
      "NEW BIO\n",
      "  oh goodness. at the moment i have 4 jobs, so it'd be nice to find one i could settle into. other than that, i'm making sure i'm surrounded by good people and keeping happy and active   i'm freakishly blonde and have the same name as that hurricane from a while back. i am always willing to try new foods and am not too picky. i do however have an extremely low tolerance to spicy food, i'm working on it tho... i've already conquered pouring chipotle hot sauce all over my burritos. guess i still have a long ways to go. one of my favorite spots in marin is sol food-- simply delicious puerto rican food.  love to laugh so i like to watch a fair amount of comedies- romantic, dark or otherwise. princess bride, pixar!, fear and loathing, caddyshack, thomas crown affair, the birdcage (\"chewing gum helps me think\"... \"sweetie, you're wasting your gum!\"), the big lebowski, star wars (don't judge)  i like historical fiction, murder mysteries, fantasy, utopian/dystopia, and, uh, the occasional romance book. just started the historian and trying to decide whether or not to read the hunger games... already saw battle royale and it was pretty cool, so i'd imagine that collin's version she ripped off has got to be fun to read.  can't get enough music. like it, love it, want some more of it. love to dance to it and to many peoples dismay, sing to it. pretty open to any kind, even country. http://www.youtube.com/watch?v=dghbozbsv18&feature=bfa&list=pl01a90d209abe1ed3&lf=plpp_video sports/my softball glove coffee. because nobody likes zombies. kindle loud music/concerts candlelight showers oh, my amazing/crazy friends and family girl scout cookies... curse those delectable treats that dethrone my generally reasonable diet-- i could eat a box of those in under an hour   in or out... drinking with friends, maybe a bar or dancing my pants off, watching or playing a game potential friends/lovers/people who come in contact with me, beware ... when alone in my car or the shower, i enjoy singing loudly and off key... and even sometimes when i'm not so alone.\n",
      "NEW BIO\n",
      "my names jake. i'm a creative guy and i look for the same in others.  i'm easy going, practical and i don't have many hang ups. i appreciate life and try to live it to the fullest. i'm sober and have been for the past few years.  i love music and i play guitar. i like tons of different bands. i'm an artist and i love to paint/draw etc. and i love creative people.  i've got to say i'm not too big on internet dating. you cant really get an earnest impression of anyone from a few polished paragraphs. but we'll see, you never know. i have an apartment. i like to explore and check things out. i like good japanese and peruvian food. nothing beats good ceviche on a hot day. or a hot chai on a cold one.  i've been working on my a.o.d. certification but have stalled out. i'm hoping to pursue art but have yet to find the best venue. recently i've been working on a construction job in belmont. it's not my dream job. but for the time being it affords me other opportunities. plus it keeps me in shape, so i shouldn't complain. i'm good at finding creative solutions to problems. i can organize a living space pretty well. i'm good at making people smile. i'm good at laughing at inappropriate times. and i make a mean bowl of cereal. i'm short i like some tv. i love summer heights high and angry boys. and i love fringe.  i'm reading stiff after finishing elliott smith and the big nothing (loved it). i like biographies.  i love music. it would be impossible to list everything i like because the list grows exponentially. i like george harrison or the clash. i like flight of the conchord's, old radiohead and elliott smith. djali zwan, x, the knitters, the kinks, john lennon, floyd, nina simone, the smiths, seu jorge, the sex pistols, immortal technique, al green, dead kennedy's, the beatles, cat stevens, nine inch nails, the dead, bob dylan etc. my taste is varied, i love music.  and i love movie's, i like all kinds. these days i'm at netflix more often though. and i never miss science friday's. music, my guitar contrast good food my bike my paintbrush my toothbrush family & friends ok....there's seven you should send a message and say hi.\n",
      "NEW BIO\n"
     ]
    }
   ],
   "source": [
    "for e in df_all.all_essays[:10]:\n",
    "    print(e)\n",
    "    print(\"NEW BIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding_array \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_essays\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:345\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    344\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 345\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    347\u001b[0m     features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:553\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[0;32m    143\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [[s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_tokenize]\n\u001b[0;32m    145\u001b[0m output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2802\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2803\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2889\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2884\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2885\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2886\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2887\u001b[0m         )\n\u001b[0;32m   2888\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2890\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2891\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2892\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2893\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2894\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2895\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2896\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2897\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2898\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2899\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2900\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2901\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2902\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2903\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2904\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2905\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2906\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2907\u001b[0m     )\n\u001b[0;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2910\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2911\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2927\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2928\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3080\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3070\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3071\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3072\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3073\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3077\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3078\u001b[0m )\n\u001b[1;32m-> 3080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3081\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3082\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3083\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3084\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3085\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3086\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3087\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3088\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3089\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3090\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3091\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3092\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3093\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3094\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3095\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3096\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3097\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3098\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    502\u001b[0m )\n\u001b[1;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    528\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_array = model.encode(df_all.loc[:, \"all_essays\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(target_vector, vectors):\n",
    "    similarities = []\n",
    "    for vector in vectors:\n",
    "        similarity = 1 - cosine(target_vector, vector)  # 1 - cosine distance to get cosine similarity\n",
    "        similarities.append(similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_matches(input_row, pref_gender=False, pref_age_lower=False, pref_age_higher=False):\n",
    "    df_possible = df_all.copy()\n",
    "    if pref_gender:\n",
    "        df_possible = df_possible.loc[df_possible.loc[:,'sex'] == pref_gender, :]\n",
    "    if pref_age_higher:\n",
    "        df_possible = df_possible[df_possible.loc[:, \"age\"] <= pref_age_higher]\n",
    "    if pref_age_lower:\n",
    "        df_possible = df_possible[df_possible.loc[:, \"age\"] >= pref_age_lower]\n",
    "\n",
    "\n",
    "    user_embeddings = embedding_array[input_row]\n",
    "\n",
    "    other_embeddings = [embedding_array[i] for i in df_possible.index]\n",
    "    # Compute the cosine similarity between the user's weighted embedding vector and all possible matches\n",
    "    cosine_similarities = compute_cosine_similarity(user_embeddings, other_embeddings)\n",
    "    # Recover index to match back to original dataframe\n",
    "    similarity_scores = [(index, score) for index, score in enumerate(cosine_similarities)]\n",
    "    # Sort by similarity\n",
    "    ranked_similarity = sorted(similarity_scores, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return ranked_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rank_matches(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about me:  i would love to think that i was some some kind of intellectual: either the dumbest smart guy, or the smartest dumb guy. can't say i can tell the difference. i love to talk about ideas and concepts. i forge odd metaphors instead of reciting cliches. like the simularities between a friend of mine's house and an underwater salt mine. my favorite word is salt by the way (weird choice i know). to me most things in life are better as metaphors. i seek to make myself a little better everyday, in some productively lazy way. got tired of tying my shoes. considered hiring a five year old, but would probably have to tie both of our shoes... decided to only wear leather shoes dress shoes.  about you:  you love to have really serious, really deep conversations about really silly stuff. you have to be willing to snap me out of a light hearted rant with a kiss. you don't have to be funny, but you have to be able to make me laugh. you should be able to bend spoons with your mind, and telepathically make me smile while i am still at work. you should love life, and be cool with just letting the wind blow. extra points for reading all this and guessing my favorite video game (no hints given yet). and lastly you have a good attention span. currently working as an international agent for a freight forwarding company. import, export, domestic you know the works. online classes and trying to better myself in my free time. perhaps a hours worth of a good book or a video game on a lazy sunday. making people laugh. ranting about a good salting. finding simplicity in complexity, and complexity in simplicity. the way i look. i am a six foot half asian, half caucasian mutt. it makes it tough not to notice me, and for me to blend in. books: absurdistan, the republic, of mice and men (only book that made me want to cry), catcher in the rye, the prince.  movies: gladiator, operation valkyrie, the producers, down periscope.  shows: the borgia, arrested development, game of thrones, monty python  music: aesop rock, hail mary mallon, george thorogood and the delaware destroyers, felt  food: i'm down for anything. food. water. cell phone. shelter. duality and humorous things trying to find someone to hang out with. i am down for anything except a club. i am new to california and looking for someone to wisper my secrets to.\n"
     ]
    }
   ],
   "source": [
    "print(df_all.loc[0, 'all_essays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i try to live my life by four two-word philosophies: do right. laugh often. work hard. care genuinely.  the pursuit of truth, open-minded approaches to the world, and not taking anything too seriously (except gym classes in elementary school and shark week) are values/traits/behaviors that i most believe in/admire in others/strive to achieve.  personal improvement and learning are very important to me and it would be great if they were important to you too. people i get along with tend to be intellectually curious about almost anything (we can start with a discussion on chaos theory and move to current events or we could begin with defensive football formations and move to the best red wine pairings. alternatively, we can talk about the best way to bake a cupcake or how to apply dijkstra's algorithm.) if you'd like a fellow intellectual spelunker, let me know... particularly if you think the phrase \"intellectual spelunker\" is funny in a childish way.  myers & briggs would call me an entj, which is dead on.  i currently love: sports, intellectual playfulness, the beach, junk food all the way to gourmet food, picnics, concerts, learning, poker, statistics, digital photography, south beath, las vegas, nyc, bowling, darts, board games, house parties, drinking games, television shows (24, house, dexter, how i met your mother, the office, arrested development, et al).  i would like to: learn to golf, learn to cook, get better at digital photography, find someone to show me new things to experience.  i'm only this intense sometimes. if you take anything away from this profile, i would hope that it is that i try to be thoughtful and put effort into things that i think are important. i am working on my mba. my dream is to one day found a new type of financial management company that focuses on financial education and literacy instead of black-box investment concepts. i want to change the world, and i think i have got the brains and gumption to do it.  before all this...  from 2005-2007, i played poker semi-professionally (end of college, beginning of real life) solely to make a living. quickly, it turned into a true passion for the nuance, beauty, and challenge of the game. eventually, i was confronted with a potentially life-altering decision to drop everything (career, friends and family on the east cost) to pursue playing professionally. i've been very lucky in my life and as a result, i have a philosophical aversion to the game's focus on preying on the weak. i came out on the side of folding my hand for good.  after that, i was a risk manager at one of the largest asset management firms (serving foundations, endowments, and pensions) in the world for the last 5 years, and now i am pursuing my mba and looking for the next big challenge.  the rest of the time i'm exploring my interests and learning as much as i can, whenever i can.  i studied computer science, applied mathematics, and economics as an undergrad and am currently working on my mba. a large percentage of what i put my mind to... i can kick ass in excel, ultimate frisbee, scrabble, being objective, strategy games of any kind (particularly board games), and playing devil's advocate.  i am a master of random trivia, and i am excellent at going with the flow.  i really like board games.  poker. that i have the most energy of any person they've met, richard simmons excluded. favorite books are freakonomics and cat's cradle. my favorite movies include fight club, the shawshank redemption, 40 year old virgin, and super bad. i have been told i have a very strange musical taste... i love a wide variety of music and my favorite artists include: lady gaga red hot chili peppers, beastie boys, madonna, goo goo dolls, 311, tupac, avril lavigne, jimmy eat world, mstrkrft, and oasis.  i eat all sorts of italian, american, and mexican food. i eat without discretion on the weekends and keep it very healthy during the week. i believe in practicality. my friends and family. great conversation. my computer and the internet. freedom. cold beer. nfl football. law and order (the show and the state of being). memory foam. warm weather. cheese. following directions. the next *big* idea. the meaning of life. philosophical questions. hypothetical situations. numbers (yeah yeah, i know...). financial markets. sports. anything with intricate systems. striving to be better each and every day. creative forms of charity. enjoying good beer, good friends, and good stories at a neighborhood bar.  headed out for a weekend trip (i like half-moon bay, tahoe, and some other places around sf and am always down for an impromptu las vegas trek)  watching movies or working on my entrepreneurial projects. i am a fairly open person. there are very few things that i would not admit to if asked.\n"
     ]
    }
   ],
   "source": [
    "print(df_all.loc[49350, 'all_essays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
